{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting Particle Markov Chain Monte Carlo\n",
    "## Introduction & Context\n",
    "\n",
    "This notebook presents the interacting particle markov chain monte carlo algorithm from http://proceedings.mlr.press/v48/rainforth16.html by Rainforth et al. (Proceedings of the 33rd International Conference on Machine Learning,). This algorithm is an extension of the family of particleMarkov chain Monte Carlo algorithms originally proposed in https://www.stats.ox.ac.uk/~doucet/andrieu_doucet_holenstein_PMCMC.pdf by Andrieu et al. .\n",
    "\n",
    "### Mathmatical context\n",
    "\n",
    "We will focus on a Markovian model even if the algorithm is not limited to this type of models.\n",
    "\n",
    "We have $x_t$ the states of our model and $y_t$ our observation following:\n",
    "- $ x_t | x_{1:t−1} ∼ f_t(x_t|x_{1:t−1}) $ the transition model \n",
    "- $ y_t | x_{1:t} ∼ g_t ( y_t | x_{1:t}) $ the observation model\n",
    "- $ x_1 ∼ \\mu(\\cdot)$\n",
    "\n",
    "Our goal is to compute expectation values with respect to the posterior distribution $ p(x_{1:T}|y_{1:T}) \\propto \\mu(x_1)\\prod\\limits_{t=2}^{T}f_t(x_t | x_{1:t-1}) \\prod\\limits_{t=1}^{T} g_t ( y_t | x_{1:t} ) $\n",
    "\n",
    "### Difficulties\n",
    "As explained in Andrieu et al., Sequential Monte Carlo (SMC) and Markov Chain Monte Carlo (MCMC) methods are often unreliable when the proposal distributions that are used to explore the space are poorly chosen and/or if highly correlated variables are updated independently. Particle Markov Chain Monte Carlo (PMCMC) try to solve theses issues using SMC methods to construct efficient proposals for the MCMC sampler. \n",
    "\n",
    "Interacting particle Markov Chain Monte Carlo tries to increase the efficiency of PMCMC, and solve its issue with *path degeneracy*.\n",
    "Whenever the ancestral lineage collapses at the early stages of the state sequence, the common ancestor is, by construction, guaranteed to be equal to the retained particle resulting in sample impoverishment, high correlation between the samples, and poor mixing of the Markov chain.\n",
    "\n",
    "To make things more concrete let's dive in the algorithm!\n",
    "\n",
    "## iPMCMC\n",
    "### Algorithm\n",
    "\n",
    "To explore the iPMCMC algorithm we must first explore SMC and CSMC, as iPMCMC uses both to generate efficient proposals of $x_{1:T}$ to the MCMC sampler.\n",
    "\n",
    "#### Sequential Monte Carlo\n",
    "\n",
    "The main idea of SMC is to generate at each time step a new position for each particle in our system, but instead of generating the next position of a particle with the particle past we take the past of any particle in our system with a discrete distribution based on the normalized weights of each one. \n",
    "\n",
    "We compute the weights of each particle at a given time with the following formula:\n",
    "\n",
    "$$ w_{t}^{i} = \\frac{g_t (y_t | x_{1:t}^{i}) f_t( x_{t}^{i}|x_{1:t-1}^{a_{t-1}^{i}} )}{ q_t(x_t^i | x_{1:t-1}^{a_{t-1}^{i}}) } $$\n",
    "\n",
    "where $g_t$ is our observation model, $f_t$ our transition model and $q_t$ our proposal.\n",
    "\n",
    "The resulting particle system approximates $p(x_{1:t}|y_{1:t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/algo1.PNG\" width=500px></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Sequential Monte Carlo\n",
    "\n",
    "CSMC is basically the same as SMC but fixing one particle trajectory. All other particles can use this fixed past but the particle itself is fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/algo2.PNG\" width=500px></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interaction Particle Markov Chain Monte Carlo\n",
    "\n",
    "IPMCMC combines both previous algorithms as MCMC sampler.\n",
    "We have M workers, P of theses M workers being CSMC samplers and M-P SMC sampler.\n",
    "At each step $r$ of the sampler all the workers generate their output according to the previously explained protocols.\n",
    "Then for each CSMC worker we randomize its output between its own and any of the SMC workers, with probabilities:\n",
    "\n",
    "Following the same notations as before, let's define:\n",
    "\n",
    "$$ \\hat{Z}_m = \\prod\\limits_{t=1}^{T}\\frac{1}{N}\\sum\\limits_{i=1}^{N}w^i_{t, m} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\zeta_{m}^{j} = \\frac{\\hat{Z}_m \\mathbb{1}_{m \\notin c_{1:P\\backslash j}} }{\\sum\\limits_{n=1}^{M} \\hat{Z}_n \\mathbb{1}_{n \\notin c_{1:P\\backslash j}}}$$\n",
    "\n",
    "to define properly the probality of *switch-in* of a given node:\n",
    "\n",
    "$$ P(c_j = m| c_{1:P\\backslash j} ) = \\hat{\\zeta}_{m}^{j} $$\n",
    "\n",
    "We then choose the fixed trajectory of each CSMC worker with a weighted randomize choice over the new outputs outputs chosen previously.\n",
    "\n",
    "$$ P(b_j = i| c_j ) = \\bar{w}_{T, c_j}^{i} $$ and $$x_{j}'[r] = x_{c_j}^{b_j}$$\n",
    "\n",
    "The output of our algorithm is the chains of all CSMC workers chosen fixed particles.\n",
    "That gives us $R \\times P$ trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/algo3.PNG\" width=500px></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from tqdm import tqdm\n",
    "from ipmcmc.generate_data import *\n",
    "from ipmcmc.linear_gaussian_state_model import *\n",
    "from ipmcmc.non_linear_gaussian_state_model import *\n",
    "from ipmcmc.smc import *\n",
    "from ipmcmc.csmc import *\n",
    "from ipmcmc.ipmcmc import *\n",
    "from ipmcmc.estimation import *    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented the same models as those in the paper. Each one of them is implemented and generates observations and states in the two next cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1. Linear Gaussian State Space Model\n",
    "np.random.seed(420)\n",
    "# Parameters\n",
    "t_max = 50\n",
    "n_particles = 50\n",
    "\n",
    "r = R.from_rotvec(np.array([7*np.pi/10, 3*np.pi/10, np.pi/20]))\n",
    "rotation_matrix = r.as_dcm()\n",
    "scaling_matrix = 0.99*np.eye(3)\n",
    "beta = np.random.dirichlet(np.ones(20)*0.2, 3).transpose()\n",
    "alpha = scaling_matrix@rotation_matrix\n",
    "t_max = 50\n",
    "mu = np.array([0, 1, 1])\n",
    "start_var = 0.1*np.eye(3)\n",
    "omega = np.eye(3)\n",
    "sigma = 0.1*np.eye(20)\n",
    "\n",
    "\n",
    "\n",
    "l_transition_model = [LinearMu(default_mean=mu, default_cov=start_var)]+[LinearTransition(\n",
    "    default_mean=np.zeros(3), default_cov=omega, default_alpha=alpha) for t in range(1, t_max)]\n",
    "l_proposals = [LinearMu(default_mean=mu, default_cov=start_var)]+[LinearProposal(\n",
    "    default_mean=np.zeros(3), default_cov=omega, default_alpha=alpha) for t in range(1, t_max)]\n",
    "l_observation_model = [LinearObservation(default_mean=np.zeros(\n",
    "    20), default_cov=sigma, default_beta=beta) for t in range(0, t_max)]\n",
    "\n",
    "# If we want to change the parameters\n",
    "assert np.all(np.linalg.eigvals(start_var) > 0)\n",
    "assert np.all(np.linalg.eigvals(omega) > 0)\n",
    "assert np.all(np.linalg.eigvals(sigma) > 0)\n",
    "\n",
    "l_states, l_observations = linear_gaussian_state_space(\n",
    "    t_max=t_max, mu=mu, start_var=start_var, transition_var=omega, noise_var=sigma,\n",
    "    transition_coeffs=alpha, observation_coeffs=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2. Nonlinear State Space Model\n",
    "np.random.seed(420)\n",
    "nl_mu = 0\n",
    "start_std = np.sqrt(5)\n",
    "omega = np.sqrt(10)\n",
    "sigma = np.sqrt(10)\n",
    "\n",
    "nl_transition_model = [NonLinearMu(default_mean=nl_mu, default_std=start_std)]+[NonLinearTransition(\n",
    "    default_mean=0, default_std=omega) for t in range(1, t_max)]\n",
    "nl_proposals = [NonLinearMu(default_mean=nl_mu, default_std=start_std)]+[\n",
    "    NonLinearProposal(default_mean=0, default_std=omega) for t in range(1, t_max)]\n",
    "nl_observation_model = [NonLinearObservation(\n",
    "    default_mean=0, default_std=sigma) for t in range(0, t_max)]\n",
    "\n",
    "nl_states, nl_observations = nonlinear_gaussian_state_space(\n",
    "    t_max=t_max, mu=nl_mu, start_std=start_std, transition_std=omega, noise_std=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_conditional_traj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:21<00:00,  1.36s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running ipmcmc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [08:05<00:00, 48.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# ipmcmc run: works with both linear and non-linear models.\n",
    "# It is pretty long to run, longer for the linear model which has 3-dimensional states.\n",
    "# For the linear model, each MCMC step take approximately 90 secs, and 80 secs for \n",
    "# the non-linear, on our computers.\n",
    "\n",
    "n_nodes = 32\n",
    "n_conditional_nodes = 16\n",
    "n_steps = 10\n",
    "\n",
    "linear= True\n",
    "\n",
    "if linear:\n",
    "    print('init_conditional_traj')\n",
    "    \n",
    "    init_conditional_traj = np.zeros((n_conditional_nodes, t_max, len(mu)))\n",
    "    for i in tqdm(range(n_conditional_nodes)):\n",
    "        \n",
    "        particles, weights, _ = smc(l_observations, n_particles,\n",
    "                              l_transition_model, l_proposals, l_observation_model)\n",
    "        b_j = np.argmax(weights[-1])\n",
    "        init_conditional_traj[i] = particles[:,b_j]\n",
    "\n",
    "    print('running ipmcmc')\n",
    "    particles, conditional_traj, weights, conditional_indices, zetas = ipmcmc(\n",
    "        n_steps, n_nodes, n_conditional_nodes, l_observations, n_particles, init_conditional_traj,\n",
    "        l_proposals, l_transition_model, l_observation_model)\n",
    "\n",
    "else:\n",
    "    print('init_conditional_traj')\n",
    "    \n",
    "    init_conditional_traj = np.zeros((n_conditional_nodes, t_max, 1))\n",
    "    for i in tqdm(range(n_conditional_nodes)):\n",
    "        particles, weights, _ = smc(nl_observations, n_particles,\n",
    "                              nl_transition_model, nl_proposals, nl_observation_model)\n",
    "        b_j = np.argmax(weights[-1])\n",
    "        init_conditional_traj[i] = particles[:,b_j]\n",
    "\n",
    "    print('running ipmcmc')\n",
    "    particles, conditional_traj, weights, conditional_indices, zetas = ipmcmc(\n",
    "        n_steps, n_nodes, n_conditional_nodes, nl_observations, n_particles, init_conditional_traj,\n",
    "        nl_proposals, nl_transition_model, nl_observation_model)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "There are several ways to see if our sampler works correctly. \n",
    "\n",
    "#### Estimation\n",
    "One one them if to estimate some parameters of the distribution, function of our retained particles from the MCMC steps $\\{\\mathbf{x}_{1:P}'[r]\\}_{r\\in\\{1,\\dots,R\\}}$:\n",
    "\n",
    "$$\\mathbb{E}[f(\\mathbf{x})] \\approx \\frac{1}{RP}\\sum\\limits_{r=1}^R\\sum\\limits_{j=1}^P f(\\mathbf{x}_j'[r])$$\n",
    "\n",
    "But in this case we would not be using all the particles generated during the rum of our iPMCMC sample. Instead we can use an estimator that uses all the particles and has a smaller variance in general. This procedure called Rao-Blackwellisation comes to replace $f(\\mathbf{x}_j'[r])$ in the above expression by\n",
    "\n",
    "$$\\mathbb{E}_{c_j|c_{1:P\\backslash j}} [\\mathbb{E}_{b_j|c_j}[\\mathbf{x}_{1:P}'[r]]] = \\sum\\limits_{m=1}^M\\hat{\\zeta}_m^j\\sum\\limits_{i=1}^N \\bar{w}_{T,m}^if(\\mathbf{x}^i_m)$$\n",
    "\n",
    "\n",
    "That is what we did to estimate the latent variables means, so $f$ is the identity function in the above formulas. We computed the error by averaging over all dimensions, since we can find the ground truth solution by running a kalman filter followed by a RTS smoothing on the observations.\n",
    "\n",
    "Since we did not have that many MCMC steps due to the computation time, we did not find the same results as in the paper, but the paper show pretty clearly how iPMCMC beats every other algorithm. If we condider the MSE function of MCMC step, taking the average over all step in the state sequence, iPMCMC beats the other algorithm at approximately 100 iterations. If we consider the MSE function of step in the state sequence for the last iteration of the MCMC algo, the iPMCMC has the best performance at the begining of the sequence, and then as similar performances to the other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean estimation for the linear model, using kalman filter and rts smoother as ground truth\n",
    "# Make sure that the particles used are the one generated during a run of the ipmcmc sampler\n",
    "# for the linear model\n",
    "\n",
    "true_means, true_covs = compute_ground_truth(l_observations, mu, start_var, alpha, omega, beta, sigma)\n",
    "\n",
    "rao_black_traj = rao_blackwellisation(particles, weights, zetas, n_conditional_nodes)\n",
    "\n",
    "errors_function_of_mcmc_step = []\n",
    "errors_function_of_state_step = []\n",
    "for r in range(1, (n_steps+1)):\n",
    "    errors_function_of_mcmc_step.append(compute_error(rao_black_traj, true_means, r))\n",
    "\n",
    "for t in range(1, (t_max+1)):\n",
    "    errors_function_of_state_step.append(compute_error(rao_black_traj, true_means, state_step=t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the particles histogram\n",
    "\n",
    "For the non linear state space model, we can not compute analytically the ground truth, so we need to find other solutions. One of them is to plot the histogram of the generated particles at a given step in the state sequence. Once again, we can see that iPMCMC outperform early the other algorithms, and has the same performances for later steps. For the ground truth we ran a kernel density estimator on combined samples from a small number of independant SMC sweeps, each with $10^7$ particles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
